[7:31 am, 14/11/2022] Priyanshu: SSIGNMENT:-3

from google.colab import drive drive.mount("/content/drive")

import numpy as np

import pandas as pd

import os

import random

import matplotlib.image import matplotlib.pyplot as plt

as mping

import seaborn as sns import cv2

import tensorflow

from keras.preprocessing.image import ImageDataGenerator

%matplotlib inline

TrainingImagePath="/content/drive/MyDrive/Image /train" TestImagePath="/content/drive/MyDrive/Image /test" ValidationImagePath="/content/drive/MyDrive/Image /valid"

train_datagen ImageDataGenerator(

rescale = 1./255, shear_range=8.1,

zoom_range=0.1, horizontal_flip=True

test_datagen = ImageDataGenerator (rescale=1./255)

training_set = train_datagen.flow_from_directory(
[7:31 am, 14/11/2022] Priyanshu: TrainingImagePath,

target_size=(128,128),

batch_size=32, class_mode="categorical"

)

test_set = test_datagen.flow_from_directory(

TestImagePath, target_size = (128,128),

batch_size=32, class_mode="categorical"

valid_set = test_datagen. flow_from_directory(

ValidationImagePath, target_size=(128,128),

batch_size=32,

class_mode="categorical"

)

def showImages(class_name):

random_index = random.choice(list(range(1,49))) folder_path= os.path.join(TrainingImagePath, class_name)

try:

image_path = os.path.join(folder_path, str(random_index).zfill(3)+".jpg") plt.imshow(mping.imread(image_path))

except:

image_path = os.path.join(folder_path, str(random_index).zfill(2)+".jpg") plt.imshow(mping. imread(image_path)) plt.title(class_name)

plt.axis(False)

plt.figure(figsize (20,20))

for labels, number in training set.class_indices.items(): plt.subplot(6,6, number+1)

showImages (labels)

test_set.class_indices
[7:32 am, 14/11/2022] Priyanshu: class_indices have the numeric tag for each balls

TrainClasses-training_set.class_indices

# Storing the face and the numeric tag for future reference

ResultMap={}

for ballvalue, ballName in zip(TrainClasses.values(), TrainClasses.keys()): ResultMap[ballValue]=ballName

import pickle

# Saving the face map for future reference with open(R"E:\Data Sets\Balls Classification\ResultsMap.pkl", 'wb') as f: pickle.dump(ResultMap, f, pickle.HIGHEST_PROTOCOL)

print("Mapping of Face and its ID",ResultMap)

#The number of neurons for the output layer is equal to the number of faces Output Neurons=len (ResultMap)

print("\n The Number of output neurons:, OutputNeurons)

from keras.models import Sequential

from keras.layers import Convolution20

from keras.layers import MaxPoo120 from keras.layers import Flatten

from keras.layers import Dense

classifier= Sequential()

classifier.add(Convolution2D(32, kernel_size=(3, 3), strides (1, 1), input_shape= (128,128,3), activation='relu'))

classifier.add(MaxPool2D(pool_size=(2,2)))

classifier.add(Convolution2D(64, kernel_size=(3, 3),
[7:32 am, 14/11/2022] Priyanshu: classifier.add(Flatten())

classifier.add(Dense(256, activation="relu"))

classifier.add(Dense (Output Neurons, activation='softmax'))

classifier.compile(loss="categorical_crossentropy', optimizer

s=["accuracy"])

'rmsprop',

metric

classifier.summary()

import time

#Measuring the time taken by the model to train StartTime-time.time()

#Starting the model training model_history=classifier.fit_generator(

training set,

steps_per_epoch-len(training set),

epochs=28,

validation_data=valid_set, verbose=1)

EndTime time.time()

validation_steps-len(valid_set),

round((EndTime-

print("**** Total Time Taken: ", StartTime)/50), 'Minutes =    ')

accuracy = model_history.history['accuracy'] val_accuracy = model_history.history['val_accuracy']

loss = model_history.history['loss'] val_loss = model_history.history['val_loss']
[7:32 am, 14/11/2022] Priyanshu: plt.figure(figsize=(15,18))

plt.subplot(2, 2, 1)

plt.plot(accuracy, label = "Training accuracy")

plt.plot(val_accuracy, label="Validation accuracy")

plt.legend() plt.title("Training vs validation accuracy")

25/57

plt.subplot(2,2,2)

plt.plot(loss, label "Training loss") plt.plot(val loss, label="Validation loss") plt.legend()

plt.title("Training vs validation loss")

plt.show()
[7:34 am, 14/11/2022] Priyanshu: Assignment 5
[7:34 am, 14/11/2022] Priyanshu: ASSIGNMENT 5:

import numpy as np

import keras.backend as K

from keras.models import Sequential

from keras.layers import Dense, Embedding, Lambda

from keras.utils import np_utils

from keras.preprocessing import sequence

from keras.preprocessing.text import Tokenizer

import gensim

data = open("/content/corona.txt","r")

covid_data= [text for text in data if text.count("")>=2]

vectorize-Tokenizer()

vectorize.fit_on_texts (covid_data)

covid_data=vectorize.texts_to_sequences(covid_data)

total_vocab=sum(len(s) for s in covid_data)

word_count=len(vectorize.word_index)+1

window_size=2

def cbow_model(data, windows size, total_vocab):

total_length=window_size*2

for text in data:

text_len-len(text)

for idx, word in enumerate(text):

context_word=[] target=[]

begin-idx-window_size

end-idx+window_size+1

context_word.append([text[1] for i in range(begin, end) if c-

ic text_len and i!=idx]) target.append(word)

contextual = sequence.pad_sequences (context_word, total_length-to

tal_length)

final_target=np_utils.to_categorical (target, total_vocab)

yield(contextual, final_target)

model-Sequential()

model.add(Embedding (input_dim-total_vocab, output_dim=100, input_length-w

indow_size*2))

model.add(Lambda(lambda x:x.mean(x,axis=1), output_shape=(100,)))

model.add(Dense(total_vocab, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam") cost-e

for i in range(10):

for x, y in cbow_model(data, window size, total_vocab): cost+=model.train_on_batch(contextual, final_target)
[7:34 am, 14/11/2022] Priyanshu: print(1, cost)

dimensions = 180

vect file-open("/content/drive/MyDrive/vector.txt", "w") vect_file.write("{}{}\n'.format(total_vocab, dimensions))

weight=model.get_weights()[0]

for text, i in vectorize.word index.items(): final vec-.join(map(str, list(weight[i,:])))

vect_file.write('(}{}\n'.format(text, final_vec))

vect_file.close()

cbow_output=gensim.models.KeyedVectors.load_word2vec_format("/content/d

rive/MyDrive/vector.txt", binary=False) cbow_output.most similar (positive=["virus"])